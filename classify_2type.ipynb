{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, glob, os\n",
    "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))\n",
    "from sparkdl import readImages\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA READ.\n",
      "DATA SPLITTED.\n",
      "TRAIN DF PREPARED.\n",
      "TEST DF PREPARED.\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"hdfs:///flower-classify/flowers\"\n",
    "\n",
    "#Read images and Create training & test DataFrames for transfer learning\n",
    "daisy_df = readImages(img_dir + \"/daisy\").withColumn(\"label\", lit(0))\n",
    "dandelion_df = readImages(img_dir + \"/dandelion\").withColumn(\"label\", lit(1))\n",
    "#roses_df = readImages(img_dir + \"/roses\").withColumn(\"label\", lit(2))\n",
    "#sunflowers_df = readImages(img_dir + \"/sunflowers\").withColumn(\"label\", lit(3))\n",
    "#tulips_df = readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(4))\n",
    "print(\"DATA READ.\")\n",
    "\n",
    "daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\n",
    "dandelion_train, dandelion_test = dandelion_df.randomSplit([0.6, 0.4])\n",
    "#roses_train, roses_test = roses_df.randomSplit([0.6, 0.4])\n",
    "#sunflowers_train, sunflowers_test = sunflowers_df.randomSplit([0.6, 0.4])\n",
    "#tulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\n",
    "print(\"DATA SPLITTED.\")\n",
    "\n",
    "#dataframe for training a classification model\n",
    "train_df = daisy_train.unionAll(dandelion_train)#.unionAll(roses_train).unionAll(sunflowers_train).unionAll(tulips_train)\n",
    "print(\"TRAIN DF PREPARED.\")\n",
    "\n",
    "#dataframe for testing the classification model\n",
    "test_df = daisy_test.unionAll(dandelion_test)#.unionAll(roses_test).unionAll(sunflowers_test).unionAll(tulips_test)\n",
    "print(\"TEST DF PREPARED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 376 variables.\n",
      "Converted 376 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "p_model = p.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 376 variables.\n",
      "Converted 376 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            filePath|               image|label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|hdfs://student11:...|[RGB,222,240,3,[B...|    0|[0.0,0.0,1.267155...|[1.72639333634102...|[0.84895050683245...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,320,3,[B...|    0|[0.0,0.3683008551...|[1.27042002554098...|[0.78081464087969...|       0.0|\n",
      "|hdfs://student11:...|[RGB,221,320,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[1.95041021409636...|[0.87549136459492...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,180,3,[B...|    0|[0.33857977390289...|[2.38786140197291...|[0.91589697898614...|       0.0|\n",
      "|hdfs://student11:...|[RGB,375,500,3,[B...|    0|[0.51109933853149...|[0.03834671975311...|[0.50958550536649...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,320,3,[B...|    0|[0.0,1.3719483613...|[1.69751485138571...|[0.84520988143920...|       0.0|\n",
      "|hdfs://student11:...|[RGB,221,320,3,[B...|    0|[0.0,0.4380842149...|[2.10541195436794...|[0.89142808037480...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,180,3,[B...|    0|[0.0,0.0,0.0,0.04...|[1.84777647114100...|[0.86386582380078...|       0.0|\n",
      "|hdfs://student11:...|[RGB,333,500,3,[B...|    0|[0.86700773239135...|[2.26005446802050...|[0.90551429133636...|       0.0|\n",
      "|hdfs://student11:...|[RGB,281,500,3,[B...|    0|[0.0,0.3926008641...|[1.28897197614250...|[0.78397313464622...|       0.0|\n",
      "|hdfs://student11:...|[RGB,215,320,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[1.84378331146639...|[0.86339553889835...|       0.0|\n",
      "|hdfs://student11:...|[RGB,375,500,3,[B...|    0|[0.0,0.0543786585...|[1.23694055821501...|[0.77503102521147...|       0.0|\n",
      "|hdfs://student11:...|[RGB,333,500,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[2.13288253860177...|[0.89405834696434...|       0.0|\n",
      "|hdfs://student11:...|[RGB,442,500,3,[B...|    0|[0.0,0.0,0.620221...|[0.43879293178369...|[0.60797137299829...|       0.0|\n",
      "|hdfs://student11:...|[RGB,292,500,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[1.98290177230516...|[0.87899015296132...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,320,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[2.02310626348409...|[0.88320182158406...|       0.0|\n",
      "|hdfs://student11:...|[RGB,240,320,3,[B...|    0|[0.0,0.0,0.0,0.0,...|[1.89258244382858...|[0.86904969885350...|       0.0|\n",
      "|hdfs://student11:...|[RGB,333,500,3,[B...|    0|[0.35632240772247...|[1.55624585073253...|[0.82581399682990...|       0.0|\n",
      "|hdfs://student11:...|[RGB,256,320,3,[B...|    0|[0.14359889924526...|[-1.8995744268842...|[0.13015664842121...|       1.0|\n",
      "|hdfs://student11:...|[RGB,441,500,3,[B...|    0|[0.0,0.0,0.0,0.56...|[1.96403694360240...|[0.87696917864808...|       0.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Training set accuracy = 0.939641109299\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "df = p_model.transform(test_df)\n",
    "df.cache()\n",
    "df.show()\n",
    "predictionAndLabels = df.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Training set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "\n",
    "#confusion_matrix = ConfusionMatrix(pandas_df.label, pandas_df.prediction)\n",
    "#print('Training set accuracy: ' + str(confusion_matrix.stats()['overall']['Accuracy']))\n",
    "#pandas_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#confusion_matrix.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
